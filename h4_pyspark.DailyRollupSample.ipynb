{"cells": [{"cell_type": "markdown", "source": "Imports, and setting up Spark sqlContext", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "outputs": [], "source": "from pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\nsqlContext = SQLContext(sc)", "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Information about Hourly Flow batches available in the Data Lake", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "outputs": [{"text": "Number of hourly batches available:  862\nFirst available batch in data lake:  201901230500\nMost recent available batch in data lake:  201902280200\n", "name": "stdout", "output_type": "stream"}], "source": "all_instances = sorted(sc._jvm.com.tetration.apps.IO.listInstances(sqlContext._ssql_ctx, \"/tetration/flows\", \"PARQUET\", \"HOURLY\"))\nprint(\"Number of hourly batches available: \", len(all_instances))\nprint(\"First available batch in data lake: \", all_instances[0])\nprint(\"Most recent available batch in data lake: \", all_instances[-1])", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "source": "### Setting Up Query Filter, Date Range, and output location and format", "metadata": {"trusted": true, "collapsed": true}}, {"execution_count": 29, "cell_type": "code", "outputs": [], "source": "from datetime import datetime, timedelta\nbatch_format = \"%Y%m%d%H00\"\nbegin_day_format = \"%Y%m%d0000\"\nend_day_format = \"%Y%m%d2300\"", "metadata": {"collapsed": true}}, {"execution_count": 30, "cell_type": "code", "outputs": [], "source": "start_from_batch = datetime.strptime(\"201902010000\", batch_format)\nend_before_batch = datetime.strptime(\"201903010000\", batch_format)", "metadata": {"collapsed": false}}, {"execution_count": 31, "cell_type": "code", "outputs": [], "source": "query_filter_selection = \"dst_address = '10.226.6.109'\"", "metadata": {"collapsed": true}}, {"execution_count": 32, "cell_type": "code", "outputs": [], "source": "daily_output_location = \"/user/daily_conversations\"\ndaily_output_format = \"PARQUET\"  # Use CSV for easier daily download.  Use PARQUET if rolling up to monthly\nmonthly_output_location = \"/user/monthly_conversations\"\nmonthly_output_format = \"CSV\"", "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Daily Aggregation\nThe following loops over each day of the month, reads 24 hours of data, filters and aggregates that data, and then writes the aggregated daily data out to the Data Lake", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "outputs": [], "source": "def get_conversations(daily_flows, query_filter):\n    daily_flows.registerTempTable(\"daily_flows\")\n    conversations = sqlContext.sql(\"\"\"\n        SELECT src_address, dst_address, dst_port, proto, count(1) as flow_count\n        FROM daily_flows\n        WHERE {}\n        GROUP BY src_address, dst_address, dst_port, proto\n    \"\"\".format(query_filter))\n    return conversations", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "outputs": [{"text": "Aggregating data for 201902010000 to 201902012300\nAggregating data for 201902020000 to 201902022300\nAggregating data for 201902030000 to 201902032300\nAggregating data for 201902040000 to 201902042300\nAggregating data for 201902050000 to 201902052300\nAggregating data for 201902060000 to 201902062300\n", "name": "stdout", "output_type": "stream"}], "source": "day_batch = start_from_batch\nwhile (day_batch < end_before_batch):\n    startBatch = datetime.strftime(day_batch, begin_day_format)\n    endBatch =  datetime.strftime(day_batch, end_day_format)\n    print(\"Aggregating data for\", startBatch, \"to\", endBatch)\n    daily_flows = sc._jvm.com.tetration.apps.IO.read(sqlContext._ssql_ctx, \"/tetration/flows\", \"PARQUET\", startBatch, endBatch)\n    selected_flows = get_conversations(daily_flows, query_filter_selection)\n    #selected_flows.orderBy(desc(\"flow_count\")).limit(10).show()\n    sc._jvm.com.tetration.apps.IO.write(selected_flows._jdf, daily_output_location, daily_output_format, datetime.strftime(day_batch, batch_format), True)\n    day_batch = day_batch + timedelta(days=1)", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "source": "### After the daily aggregations are written, then a monthly aggregation can be computed\nAggregation columns need to correspond with daily aggregation columns.  If the above select statement in `get_conversations` is changes, then the select statement below needs to be changed as well.", "metadata": {}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "month_batch_start = datetime.strftime(start_from_batch, batch_format)\nmonth_batch_end = datetime.strftime(end_before_batch - timedelta(days=1), batch_format)\nprint(\"Aggregating daily batches from {} to {}\".format(month_batch_start, month_batch_end))\nmonthly_flows = sc._jvm.com.tetration.apps.IO.read(sqlContext._ssql_ctx, daily_output_location, daily_output_format, month_batch_start, month_batch_end)\nmonthly_flows.registerTempTable(\"monthly_flows\")\nmonthly_flows_agg = sqlContext.sql(\"\"\"\n        SELECT src_address, dst_address, dst_port, proto, sum(flow_count) as flow_count\n        FROM monthly_flows\n        GROUP BY src_address, dst_address, dst_port, proto\n    \"\"\")\nsc._jvm.com.tetration.apps.IO.write(monthly_flows_agg._jdf, monthly_output_location, monthly_output_format, month_batch_start, True)", "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "", "metadata": {"collapsed": true}}], "nbformat_minor": 2, "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.5.4", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "PySpark", "name": "h4_pyspark", "language": "python"}}, "nbformat": 4}